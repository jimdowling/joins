{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "import sys\n",
    "\n",
    "#num_rows=100000\n",
    "num_rows=1000000\n",
    "#num_rows=10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f1\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg1=spark.read.csv(hdfs.project_path() + \"Resources/\" + str(num_rows) + \"-20-1-out.csv\", header=True, schema=fg1_schema)\n",
    "fg1=fg1.sort(col(\"id\"),col(\"ts\"))\n",
    "fg1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# fg2_schema = StructType([\n",
    "#   StructField(\"id\", IntegerType(), True),\n",
    "#   StructField(\"ts\", IntegerType(), True),\n",
    "#   StructField(\"f2\", StringType(), True)\n",
    "# ])\n",
    "# , schema=fg2_schema\n",
    "fg2=spark.read.csv(hdfs.project_path() + \"Resources/\" + str(num_rows) + \"-20-2-out.csv\", header=True)\n",
    "fg2=fg2.sort(col(\"id\"),col(\"ts\"))\n",
    "fg2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "maprdd = fg2.rdd.groupBy(lambda x:x[0]).map(lambda x:(x[0],{y[1] for y in x[1]}))\n",
    "result_dict = dict(maprdd.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "map_broadcast = sc.broadcast(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "def take_closest(id, ts):\n",
    "    return min(map_broadcast.value[id],key=lambda x:((ts-x) if ts >= x else sys.maxsize))\n",
    "columns = [\"id2\",\"join_ts\",\"ts2\"]\n",
    "\n",
    "my_rdd = fg1.rdd.map(lambda x: (x[0],x[1],take_closest(x[0],x[1])))\n",
    "filtered_rdd = my_rdd.filter(lambda x: x[1] >= x[2])\n",
    "join2=spark.createDataFrame(filtered_rdd,columns).sort(col(\"id2\"),col(\"join_ts\"))\n",
    "join2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "joined = fg1.join(join2,(fg1.id==join2.id2) & (fg1.ts==join2.join_ts),how=\"inner\")\n",
    "drop_cols = ['id2', 'join_ts']\n",
    "inter = joined.drop(*drop_cols)\n",
    "inter.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "inter = inter.alias('inter')\n",
    "fg2 = fg2.alias('fg2')\n",
    "final = inter.join(fg2,(inter.id==fg2.id) & (inter.ts2==fg2.ts),how=\"inner\")\n",
    "#.select('inter.id', 'inter.ts', 'inter.f1', 'fg2.f2')\n",
    "final.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-10 22:07:18,917 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-10 22:07:19,031 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-10 22:07:19,210 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.5:8032\n",
      "2021-02-10 22:07:20,151 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-02-10 22:07:20,264 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-10 22:07:20,274 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-10 22:07:20,291 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-10 22:07:20,292 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-10 22:07:20,292 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-10 22:07:20,308 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-10 22:07:20,328 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-10 22:07:21,058 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-10 22:07:21,147 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-10 22:07:21,157 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-10 22:07:21,406 INFO  Client: Uploading resource file:/tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6/__spark_conf__2582678470469565778.zip -> hdfs:/Projects/demo_fs_meb10179/Resources/.sparkStaging/application_1612962379410_0001/__spark_conf__.zip\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing view acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,945 INFO  SecurityManager: Changing modify acls to: livy,demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:21,946 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-10 22:07:21,947 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_fs_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_fs_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-10 22:07:22,067 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-10 22:07:23,737 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-10 22:07:23,756 INFO  Client: Submitting application application_1612962379410_0001 to ResourceManager\n",
      "2021-02-10 22:07:24,076 INFO  YarnClientImpl: Submitted application application_1612962379410_0001\n",
      "2021-02-10 22:07:24,079 INFO  Client: Application report for application_1612962379410_0001 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-10 22:07:24,082 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612994843793\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://jimeuwester-master.internal.cloudapp.net:8088/proxy/application_1612962379410_0001/\n",
      "\t user: demo_fs_meb10179__meb10179\n",
      "2021-02-10 22:07:24,087 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-10 22:07:24,088 INFO  ShutdownHookManager: Deleting directory /tmp/spark-418cf707-015d-45f2-92c3-bfd7acdedc9f\n",
      "2021-02-10 22:07:24,093 INFO  ShutdownHookManager: Deleting directory /tmp/spark-40434ac5-d5cf-4517-aabb-dcc7b440c9b6\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
