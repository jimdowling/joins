{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>6</td><td>application_1612348338568_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1612348338568_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworksjimeurope-worker-9.internal.cloudapp.net:8042/node/containerlogs/container_e02_1612348338568_0009_01_000001/demo_ml_meb10179__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "import sys\n",
    "\n",
    "#num_rows=100000\n",
    "num_rows=1000000\n",
    "#num_rows=10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|  ts| f1|\n",
      "+---+----+---+\n",
      "|  1|1020| f1|\n",
      "|  1|1040| f1|\n",
      "|  1|1060| f1|\n",
      "|  1|1080| f1|\n",
      "+---+----+---+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f1\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg1=spark.read.csv(hdfs.project_path() + \"Resources/\" + str(num_rows) + \"-20-1-out.csv\", header=True, schema=fg1_schema)\n",
    "fg1=fg1.sort(col(\"id\"),col(\"ts\"))\n",
    "fg1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000000"
     ]
    }
   ],
   "source": [
    "fg1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+---+---+---+--------+--------------------+----------------+\n",
      "| id|  ts| f2| g2| h2| i2| j2|'number'|               'msg'|'another_number'|\n",
      "+---+----+---+---+---+---+---+--------+--------------------+----------------+\n",
      "|  1|1020|  1|  1| h2| i2| j2| 3333333|'some string that...|         9999999|\n",
      "|  1|1040|  1|  1| h2| i2| j2| 3333333|'some string that...|         9999999|\n",
      "|  1|1060|  1|  1| h2| i2| j2| 3333333|'some string that...|         9999999|\n",
      "|  1|1080|  1|  1| h2| i2| j2| 3333333|'some string that...|         9999999|\n",
      "+---+----+---+---+---+---+---+--------+--------------------+----------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "# fg2_schema = StructType([\n",
    "#   StructField(\"id\", IntegerType(), True),\n",
    "#   StructField(\"ts\", IntegerType(), True),\n",
    "#   StructField(\"f2\", StringType(), True)\n",
    "# ])\n",
    "# , schema=fg2_schema\n",
    "fg2=spark.read.csv(hdfs.project_path() + \"Resources/\" + str(num_rows) + \"-20-2-out.csv\", header=True)\n",
    "fg2=fg2.sort(col(\"id\"),col(\"ts\"))\n",
    "fg2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000"
     ]
    }
   ],
   "source": [
    "fg2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maprdd = fg2.rdd.groupBy(lambda x:x[0]).map(lambda x:(x[0],{y[1] for y in x[1]}))\n",
    "result_dict = dict(maprdd.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Interpreter died:\n",
      "\n",
      "/srv/hops/spark/python/lib/pyspark.zip/pyspark/context.py:252: RuntimeWarning: Failed to add file [local:///srv/hops/spark/python/lib/pyspark.zip] speficied in 'spark.submit.pyFiles' to Python path:\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/tmp\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/spark-6f9d1294-4ca7-4341-8547-8e98cc295afd/userFiles-5cc9a9d2-6ad2-4fe2-a04c-b59123598080\n",
      "  /srv/hops/spark/python/lib/pyspark.zip\n",
      "  /srv/hops/spark/python/lib/py4j-src.zip\n",
      "  /srv/hops/spark/python/lib/py4j-0.10.7-src.zip\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/pyspark.zip\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/py4j-src.zip\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python37.zip\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7/lib-dynload\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages\n",
      "  RuntimeWarning)\n",
      "/srv/hops/spark/python/lib/pyspark.zip/pyspark/context.py:252: RuntimeWarning: Failed to add file [local:///srv/hops/spark/python/lib/py4j-src.zip] speficied in 'spark.submit.pyFiles' to Python path:\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/tmp\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/spark-6f9d1294-4ca7-4341-8547-8e98cc295afd/userFiles-5cc9a9d2-6ad2-4fe2-a04c-b59123598080\n",
      "  /srv/hops/spark/python/lib/pyspark.zip\n",
      "  /srv/hops/spark/python/lib/py4j-src.zip\n",
      "  /srv/hops/spark/python/lib/py4j-0.10.7-src.zip\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/pyspark.zip\n",
      "  /srv/hops/hopsdata/tmp/nm-local-dir/usercache/demo_ml_meb10179__meb10179/appcache/application_1612348338568_0009/container_e02_1612348338568_0009_01_000001/py4j-src.zip\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python37.zip\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7/lib-dynload\n",
      "  /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages\n",
      "  RuntimeWarning)\n",
      "\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.fs.FileSystem).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_broadcast = sc.broadcast(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 6 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2021-02-03 15:58:39,852 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-03 15:58:39,944 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-03 15:58:40,068 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-03 15:58:40,924 INFO  Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "2021-02-03 15:58:41,012 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-03 15:58:41,023 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-03 15:58:41,045 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-03 15:58:41,046 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-03 15:58:41,046 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-03 15:58:41,052 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-03 15:58:41,064 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-03 15:58:41,988 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-03 15:58:42,110 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-03 15:58:42,119 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-03 15:58:42,361 INFO  Client: Uploading resource file:/tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651/__spark_conf__2795220765480263209.zip -> hdfs:/Projects/demo_ml_meb10179/Resources/.sparkStaging/application_1612348338568_0009/__spark_conf__.zip\n",
      "2021-02-03 15:58:42,996 INFO  SecurityManager: Changing view acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,997 INFO  SecurityManager: Changing modify acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-03 15:58:42,999 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_ml_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_ml_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-03 15:58:43,074 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-03 15:58:44,381 INFO  Client: Submitting application application_1612348338568_0009 to ResourceManager\n",
      "2021-02-03 15:58:44,442 INFO  YarnClientImpl: Submitted application application_1612348338568_0009\n",
      "2021-02-03 15:58:44,446 INFO  Client: Application report for application_1612348338568_0009 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-03 15:58:44,450 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612367924401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1612348338568_0009/\n",
      "\t user: demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:44,457 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-03 15:58:44,458 INFO  ShutdownHookManager: Deleting directory /tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651\n",
      "2021-02-03 15:58:44,465 INFO  ShutdownHookManager: Deleting directory /tmp/spark-66127f65-38b8-4c85-b562-b68b6ed7fc01\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "def take_closest(id, ts):\n",
    "    return min(map_broadcast.value[id],key=lambda x:((ts-x) if ts >= x else sys.maxsize))\n",
    "columns = [\"id2\",\"join_ts\",\"ts2\"]\n",
    "\n",
    "my_rdd = fg1.rdd.map(lambda x: (x[0],x[1],take_closest(x[0],x[1])))\n",
    "filtered_rdd = my_rdd.filter(lambda x: x[1] >= x[2])\n",
    "join2=spark.createDataFrame(filtered_rdd,columns).sort(col(\"id2\"),col(\"join_ts\"))\n",
    "join2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 6 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2021-02-03 15:58:39,852 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-03 15:58:39,944 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-03 15:58:40,068 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-03 15:58:40,924 INFO  Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "2021-02-03 15:58:41,012 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-03 15:58:41,023 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-03 15:58:41,045 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-03 15:58:41,046 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-03 15:58:41,046 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-03 15:58:41,052 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-03 15:58:41,064 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-03 15:58:41,988 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-03 15:58:42,110 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-03 15:58:42,119 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-03 15:58:42,361 INFO  Client: Uploading resource file:/tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651/__spark_conf__2795220765480263209.zip -> hdfs:/Projects/demo_ml_meb10179/Resources/.sparkStaging/application_1612348338568_0009/__spark_conf__.zip\n",
      "2021-02-03 15:58:42,996 INFO  SecurityManager: Changing view acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,997 INFO  SecurityManager: Changing modify acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-03 15:58:42,999 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_ml_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_ml_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-03 15:58:43,074 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-03 15:58:44,381 INFO  Client: Submitting application application_1612348338568_0009 to ResourceManager\n",
      "2021-02-03 15:58:44,442 INFO  YarnClientImpl: Submitted application application_1612348338568_0009\n",
      "2021-02-03 15:58:44,446 INFO  Client: Application report for application_1612348338568_0009 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-03 15:58:44,450 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612367924401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1612348338568_0009/\n",
      "\t user: demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:44,457 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-03 15:58:44,458 INFO  ShutdownHookManager: Deleting directory /tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651\n",
      "2021-02-03 15:58:44,465 INFO  ShutdownHookManager: Deleting directory /tmp/spark-66127f65-38b8-4c85-b562-b68b6ed7fc01\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "joined = fg1.join(join2,(fg1.id==join2.id2) & (fg1.ts==join2.join_ts),how=\"inner\")\n",
    "drop_cols = ['id2', 'join_ts']\n",
    "inter = joined.drop(*drop_cols)\n",
    "inter.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 6 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2021-02-03 15:58:39,852 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-03 15:58:39,944 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-03 15:58:40,068 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-03 15:58:40,924 INFO  Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "2021-02-03 15:58:41,012 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-03 15:58:41,023 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-03 15:58:41,045 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-03 15:58:41,046 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-03 15:58:41,046 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-03 15:58:41,052 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-03 15:58:41,064 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-03 15:58:41,988 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-03 15:58:42,110 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-03 15:58:42,119 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-03 15:58:42,361 INFO  Client: Uploading resource file:/tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651/__spark_conf__2795220765480263209.zip -> hdfs:/Projects/demo_ml_meb10179/Resources/.sparkStaging/application_1612348338568_0009/__spark_conf__.zip\n",
      "2021-02-03 15:58:42,996 INFO  SecurityManager: Changing view acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,997 INFO  SecurityManager: Changing modify acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-03 15:58:42,999 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_ml_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_ml_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-03 15:58:43,074 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-03 15:58:44,381 INFO  Client: Submitting application application_1612348338568_0009 to ResourceManager\n",
      "2021-02-03 15:58:44,442 INFO  YarnClientImpl: Submitted application application_1612348338568_0009\n",
      "2021-02-03 15:58:44,446 INFO  Client: Application report for application_1612348338568_0009 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-03 15:58:44,450 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612367924401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1612348338568_0009/\n",
      "\t user: demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:44,457 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-03 15:58:44,458 INFO  ShutdownHookManager: Deleting directory /tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651\n",
      "2021-02-03 15:58:44,465 INFO  ShutdownHookManager: Deleting directory /tmp/spark-66127f65-38b8-4c85-b562-b68b6ed7fc01\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "inter = inter.alias('inter')\n",
    "fg2 = fg2.alias('fg2')\n",
    "final = inter.join(fg2,(inter.id==fg2.id) & (inter.ts2==fg2.ts),how=\"inner\")\n",
    "#.select('inter.id', 'inter.ts', 'inter.f1', 'fg2.f2')\n",
    "final.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 6 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "2021-02-03 15:58:39,852 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-03 15:58:39,944 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-03 15:58:40,068 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-03 15:58:40,924 INFO  Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "2021-02-03 15:58:41,012 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-03 15:58:41,023 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-03 15:58:41,045 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-03 15:58:41,046 INFO  Client: Will allocate AM container, with 17652 MB memory including 1604 MB overhead\n",
      "2021-02-03 15:58:41,046 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-03 15:58:41,052 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-03 15:58:41,064 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-03 15:58:41,988 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-03 15:58:42,110 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-03 15:58:42,119 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-03 15:58:42,361 INFO  Client: Uploading resource file:/tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651/__spark_conf__2795220765480263209.zip -> hdfs:/Projects/demo_ml_meb10179/Resources/.sparkStaging/application_1612348338568_0009/__spark_conf__.zip\n",
      "2021-02-03 15:58:42,996 INFO  SecurityManager: Changing view acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,997 INFO  SecurityManager: Changing modify acls to: livy,demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-03 15:58:42,998 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-03 15:58:42,999 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, demo_ml_meb10179__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, demo_ml_meb10179__meb10179); groups with modify permissions: Set()\n",
      "2021-02-03 15:58:43,074 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-03 15:58:44,375 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-03 15:58:44,381 INFO  Client: Submitting application application_1612348338568_0009 to ResourceManager\n",
      "2021-02-03 15:58:44,442 INFO  YarnClientImpl: Submitted application application_1612348338568_0009\n",
      "2021-02-03 15:58:44,446 INFO  Client: Application report for application_1612348338568_0009 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-03 15:58:44,450 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612367924401\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1612348338568_0009/\n",
      "\t user: demo_ml_meb10179__meb10179\n",
      "2021-02-03 15:58:44,457 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-03 15:58:44,458 INFO  ShutdownHookManager: Deleting directory /tmp/spark-8c45c0aa-4e5e-4347-8461-fea192663651\n",
      "2021-02-03 15:58:44,465 INFO  ShutdownHookManager: Deleting directory /tmp/spark-66127f65-38b8-4c85-b562-b68b6ed7fc01\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
